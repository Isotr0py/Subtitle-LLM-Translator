{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1sLecGR9k7q"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Isotr0py/Sakura-Subtitle/blob/main/whisper/Faster-Whisper-Colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-_mAy9ftPZA",
        "outputId": "7d96fb84-2357-42f8-fe67-c66a4a327502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "Mon Jan 22 14:20:00 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@title 初始化环境\n",
        "#@markdown 挂载Google网盘\n",
        "from pathlib import Path\n",
        "Mount_GDrive = True # @param {type:\"boolean\"}\n",
        "if Mount_GDrive:\n",
        "  from google.colab import drive\n",
        "\n",
        "  drive.mount('/content/gdrive')\n",
        "  ROOT_PATH = \"/content/gdrive/MyDrive\"\n",
        "else:\n",
        "  ROOT_PATH = \"/content\"\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "72-0wL8IUmmY"
      },
      "outputs": [],
      "source": [
        "#@title 安装faster-whisper\n",
        "from IPython.display import clear_output\n",
        "!pip install -q faster-whisper\n",
        "!sudo apt-get update\n",
        "!sudo apt install -qq nvidia-cuda-toolkit\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31El8nm_uFRo",
        "outputId": "46d67e2f-adac-4563-d32d-cdbb9a7f88c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Media\n"
          ]
        }
      ],
      "source": [
        "#@title 设置工作区\n",
        "#@markdown 音频文件存放文件夹\n",
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "\n",
        "work_dir = \"Media/\" # @param {type:\"string\"}\n",
        "work_dir = Path(ROOT_PATH).joinpath(work_dir)\n",
        "work_dir.mkdir(parents=True, exist_ok=True)\n",
        "%cd $work_dir\n",
        "\n",
        "#@markdown 上传文件至工作区\n",
        "upload_file = False  # @param {type:\"boolean\"}\n",
        "if upload_file:\n",
        "  uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lcdbVyRGXK_f"
      },
      "outputs": [],
      "source": [
        "#@title 语音转录\n",
        "from datetime import datetime\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "\n",
        "#@markdown 模型设置\n",
        "model_size = \"large-v3\" #@param [\"large-v1\", \"large-v2\", \"large-v3\"]\n",
        "device = \"cuda\" #@param [\"cpu\", \"cuda\"]\n",
        "\n",
        "#@markdown 转录设置\n",
        "language = \"ja\" #@param [\"auto\", \"en\", \"zh\", \"ja\"]\n",
        "language = None if language == \"auto\" else language\n",
        "beam_size = 5 # @param {type:\"integer\"}\n",
        "vad_filter = True #@param {type:\"boolean\"}\n",
        "\n",
        "min_speech_duration_ms = 250 # @param {type:\"integer\"}\n",
        "min_silence_duration_ms = 2000 # @param {type:\"integer\"}\n",
        "vad_parameters = {\n",
        "  \"min_speech_duration_ms\": min_speech_duration_ms,\n",
        "  \"min_silence_duration_ms\": min_silence_duration_ms,\n",
        "}\n",
        "\n",
        "#@markdown 输出设置\n",
        "output_dir = \"./outputs/\" #@param {type:\"string\"}\n",
        "output_format = \"vtt\" #@param [\"lrc\", \"vtt\"]\n",
        "\n",
        "# Run on GPU with FP16\n",
        "model = WhisperModel(model_size, device=device)\n",
        "\n",
        "files = glob(\"*.wav\")+glob(\"*.mp3\")+glob(\"*.flac\")\n",
        "for file in files:\n",
        "  if output_format == \"lrc\":\n",
        "    output_file = Path(output_dir).joinpath(f\"{Path(file).stem}.{output_format}\")\n",
        "  if output_format == \"vtt\":\n",
        "    output_file = Path(output_dir).joinpath(f\"{Path(file).name}.{output_format}\")\n",
        "  Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "  segments, info = model.transcribe(file, language=language, beam_size=5, vad_filter=True, vad_parameters=vad_parameters)\n",
        "  print(\"%s: Detected language '%s' with probability %f\" % (file, info.language, info.language_probability))\n",
        "\n",
        "  with open(output_file,\"w\") as f:\n",
        "    if output_format == \"lrc\":\n",
        "      f.write(\"[Transcribed by Whisper]\\n\")\n",
        "    if output_format == \"vtt\":\n",
        "      f.write(\"WEBVTT - Transcribed by Whisper\\n\\n\")\n",
        "    for segment in segments:\n",
        "        text = segment.text\n",
        "        start= datetime.fromtimestamp(segment.start).strftime(\"%H:%M:%S.%f\")[:-3]\n",
        "        end = datetime.fromtimestamp(segment.end).strftime(\"%H:%M:%S.%f\")[:-3]\n",
        "        print(f\"[{start} --> {end}] {text}\")\n",
        "        if output_format == \"lrc\":\n",
        "          start= datetime.fromtimestamp(segment.start).strftime(\"%M:%S.%f\")[:-3]\n",
        "          end = datetime.fromtimestamp(segment.end).strftime(\"%M:%S.%f\")[:-3]\n",
        "          f.write(f\"[{start}]{text}\\n[{end}]\\n\")\n",
        "        if output_format == \"vtt\":\n",
        "          start= datetime.fromtimestamp(segment.start).strftime(\"%H:%M:%S.%f\")[:-3]\n",
        "          end = datetime.fromtimestamp(segment.end).strftime(\"%H:%M:%S.%f\")[:-3]\n",
        "          f.write(f\"{start} --> {end}\\n{text}\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
